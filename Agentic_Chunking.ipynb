{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Init Agentic Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from IPython.display import Markdown\n",
    "from phi.agent import Agent\n",
    "from phi.model.ollama import Ollama\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "from utils.document_processor import DocumentProcessor  \n",
    "\n",
    "# Define constant paths and configuration\n",
    "DATA_PATH = \"./data\"               # Directory containing input documents\n",
    "INDEX_PATH = \"faiss_index\"         # Directory for storing FAISS index\n",
    "CHUNKED_DATA_PATH = \"./chunked_data\"   # Directory for storing chunked text\n",
    "METADATA_PATH = \"./metadata\"           # Directory for storing document metadata\n",
    "OLLAMA_MODEL = \"llama3.2\"             # Specify the LLM model to use\n",
    "\n",
    "# Create necessary directories if they don't exist\n",
    "os.makedirs(CHUNKED_DATA_PATH, exist_ok=True)\n",
    "os.makedirs(METADATA_PATH, exist_ok=True)\n",
    "\n",
    "# Initialize the language model and processing components\n",
    "llm = Ollama(id=OLLAMA_MODEL)                                     # Initialize LLM\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")  # Initialize embedding model\n",
    "docs = DocumentProcessor()                                         # Initialize document processor\n",
    "agent = Agent(model=llm, show_tool_calls=True, markdown=True)     # Initialize agent for text processing\n",
    "\n",
    "# Configure chunking parameters\n",
    "CHUNK_SIZE = 1200      # Size of each text chunk in characters\n",
    "MIN_CHUNK_SIZE = 500   # Minimum chunk size before merging\n",
    "MAX_CHUNKS = 30        # Maximum number of chunks per document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Dokumen (1).pdf - Panjang teks sebelum pemrosesan: 3843\n",
      "[INFO] Total chunks generated for Dokumen (1).pdf: 3\n"
     ]
    }
   ],
   "source": [
    "# List to store processed documents\n",
    "extracted_docs = []\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean input text by removing excess whitespace and non-ASCII characters\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to clean\n",
    "    Returns:\n",
    "        str: Cleaned text\n",
    "    \"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Replace multiple spaces with single space\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)  # Remove non-ASCII characters\n",
    "    return text\n",
    "\n",
    "def clean_agent_output(text):\n",
    "    \"\"\"\n",
    "    Clean the output from the agent by removing unnecessary markdown and formatting\n",
    "    \n",
    "    Args:\n",
    "        text (str): Agent output text to clean\n",
    "    Returns:\n",
    "        str: Cleaned text without markdown formatting\n",
    "    \"\"\"\n",
    "    text = re.sub(r'\\n?###.*?\\n', '\\n', text)  # Remove markdown headers\n",
    "    text = re.sub(r'\\n?\\*\\*\\*.*?\\n', '\\n', text)  # Remove asterisk separators\n",
    "    text = re.sub(r'\\n?-{3,}\\n?', '\\n', text)  # Remove dash separators\n",
    "    text = re.sub(r'(\\s*-{2,}\\s*)', ' ', text)  # Remove double dashes\n",
    "    text = re.sub(r'(\\s*\\*{2,}\\s*)', ' ', text)  # Remove bold markers\n",
    "    text = re.sub(r'(\\s*\\*\\s*)', ' ', text)  # Remove single asterisks\n",
    "    text = re.sub(r'(\\s*-\\s*)', ' ', text)  # Remove single dashes\n",
    "    text = re.sub(r'\\n{2,}', '\\n\\n', text).strip()  # Normalize line breaks\n",
    "    return text\n",
    "\n",
    "# Main document processing loop\n",
    "for filename in os.listdir(DATA_PATH):\n",
    "    # Check if file has valid extension\n",
    "    valid_extensions = ('.pdf', '.docx', '.txt')\n",
    "    if not filename.lower().endswith(valid_extensions):\n",
    "        continue\n",
    "\n",
    "    filepath = os.path.join(DATA_PATH, filename)\n",
    "\n",
    "    try:\n",
    "        # Read and process the document\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            document = f.read()\n",
    "            result = docs.process_document(document, filename)\n",
    "\n",
    "        # Skip if document processing failed\n",
    "        if not result or len(result) < 4:\n",
    "            print(f\"[WARNING] Gagal memproses {filename}, melewati file ini.\")\n",
    "            continue\n",
    "\n",
    "        # Clean and prepare text for chunking\n",
    "        plain_text = clean_text(result[3])  \n",
    "        print(f\"[INFO] {filename} - Panjang teks sebelum pemrosesan: {len(plain_text)}\")\n",
    "\n",
    "        # Process text in chunks using the agent\n",
    "        structured_text = \"\"\n",
    "        start_idx = 0\n",
    "        chunk_count = 0\n",
    "\n",
    "        # Iterate through text in chunks\n",
    "        while start_idx < len(plain_text) and chunk_count < MAX_CHUNKS:\n",
    "            chunk_text = plain_text[start_idx:start_idx + CHUNK_SIZE]\n",
    "            response = agent.run(\n",
    "                f\"Split the following text into meaningful segments ensuring logical separation:\\n{chunk_text}\",\n",
    "                max_tokens=8000\n",
    "            )\n",
    "            \n",
    "            # Handle different response types from agent\n",
    "            if isinstance(response, str):\n",
    "                structured_text += clean_agent_output(response) + \"\\n\\n\"\n",
    "            elif isinstance(response, dict):\n",
    "                structured_text += clean_agent_output(response.get(\"text\", \"\")) + \"\\n\\n\"\n",
    "            else:\n",
    "                structured_text += clean_agent_output(getattr(response, \"content\", str(response))) + \"\\n\\n\"\n",
    "            \n",
    "            start_idx += CHUNK_SIZE\n",
    "            chunk_count += 1\n",
    "\n",
    "        # Process and optimize chunks\n",
    "        structured_text = structured_text.strip()\n",
    "        chunked_texts = structured_text.split(\"\\n\\n\")\n",
    "\n",
    "        # Combine small chunks to meet minimum size requirement\n",
    "        optimized_chunks = []\n",
    "        temp_chunk = \"\"\n",
    "\n",
    "        for chunk in chunked_texts:\n",
    "            chunk = chunk.strip()\n",
    "            if len(chunk) < MIN_CHUNK_SIZE:\n",
    "                temp_chunk += \" \" + chunk\n",
    "            else:\n",
    "                if temp_chunk:\n",
    "                    optimized_chunks.append(temp_chunk.strip())\n",
    "                    temp_chunk = \"\"\n",
    "                optimized_chunks.append(chunk)\n",
    "\n",
    "        if temp_chunk:\n",
    "            optimized_chunks.append(temp_chunk.strip())\n",
    "\n",
    "        # Create chunk data structure with metadata\n",
    "        chunk_data = [{\"chunk_id\": i+1, \"text\": chunk.strip()} \n",
    "                      for i, chunk in enumerate(optimized_chunks[:MAX_CHUNKS]) if chunk.strip()]\n",
    "\n",
    "        # Create metadata for the document\n",
    "        metadata = {\n",
    "            \"filename\": filename,\n",
    "            \"total_chunks\": len(chunk_data),\n",
    "            \"total_length\": len(plain_text)\n",
    "        }\n",
    "\n",
    "        # Create Document objects for vector store\n",
    "        extracted_docs.extend([\n",
    "            Document(page_content=chunk[\"text\"], metadata={\"chunk_id\": chunk[\"chunk_id\"], **metadata}) \n",
    "            for chunk in chunk_data\n",
    "        ])\n",
    "\n",
    "        # Save chunked text to file\n",
    "        chunked_filepath = os.path.join(CHUNKED_DATA_PATH, f\"chunked_{filename}.txt\")\n",
    "        with open(chunked_filepath, \"w\", encoding=\"utf-8\") as chunked_file:\n",
    "            for chunk in chunk_data:\n",
    "                chunked_file.write(f\"Chunk {chunk['chunk_id']}:\\n\")\n",
    "                chunked_file.write(f\"{chunk['text']}\\n\")\n",
    "                chunked_file.write(\"\\n---\\n\\n\")  \n",
    "\n",
    "        # Save metadata to separate JSON file\n",
    "        metadata_filepath = os.path.join(METADATA_PATH, f\"metadata_{filename}.json\")\n",
    "        with open(metadata_filepath, \"w\", encoding=\"utf-8\") as metadata_file:\n",
    "            json.dump(metadata, metadata_file, indent=4)\n",
    "\n",
    "        print(f\"[INFO] Total chunks generated for {filename}: {len(chunk_data)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Error processing {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCCESS] Proses chunking selesai. Hasilnya disimpan dalam 'chunked_data' dan metadata di 'metadata'.\n"
     ]
    }
   ],
   "source": [
    "# Save processed documents to FAISS vector store if any exist\n",
    "if extracted_docs:\n",
    "    vector_store = FAISS.from_documents(extracted_docs, embedding_model)\n",
    "    vector_store.save_local(INDEX_PATH)\n",
    "    print(\"[SUCCESS] Proses chunking selesai. Hasilnya disimpan dalam 'chunked_data' dan metadata di 'metadata'.\")\n",
    "else:\n",
    "    print(\"[INFO] Tidak ada dokumen yang berhasil diproses.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
